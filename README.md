# Qwen-Finetuning

Qwen-Finetuning is a project that enables fine-tuning of Qwen vision-language models, such as Qwen-VL, using the Llama-Factory framework. It provides a streamlined process for adapting these models to your specific tasks and datasets, with support for efficient training techniques and experiment tracking via Weights & Biases.

## Features

- Fine-tune Qwen vision-language models (e.g., Qwen-VL) on custom datasets.
- Utilize the Llama-Factory framework for efficient and user-friendly fine-tuning.
- Track experiments and monitor performance with Weights & Biases.
- Support for various fine-tuning techniques such as LoRA and QLoRA.

## Prerequisites

- Python 3.8 or higher
- PyTorch
- Transformers library
- Llama-Factory framework
- Access to a GPU (recommended for faster training)
- A Weights & Biases account for experiment tracking

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/Anakintano/Qwen-Finetuning.git
   cd Qwen-Finetuning


Ensure that the Llama-Factory framework is installed. Refer to the Llama-Factory documentation for installation instructions.

## Getting Started
To quickly get started with fine-tuning a Qwen model:
Prepare your dataset in the format required by Llama-Factory.

Configure the training parameters as needed.

Run the fine-tuning process using the provided scripts or notebooks.

## Weights & Biases (wandb)
This project integrates with Weights & Biases for experiment tracking. To use wandb:
Sign up or log in to your Weights & Biases account at https://wandb.ai/login.

Go to your settings page and find your API key under "API keys."

Copy the API key and set it in your environment or configuration file as required by the project.

For more information, visit the Weights & Biases documentation.

## Contributing
Contributions are welcome! Please feel free to submit issues or pull requests to the repository.

## License
This project is open-source and licensed under the MIT License


Anakintano | 2025











